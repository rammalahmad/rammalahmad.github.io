---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Greetings, I'm Ahmad Rammal, a dedicated individual with a profound enthusiasm for mathematics, machine learning, and ethics. With a background in Applied Mathematics from [Ecole polytechnique](https://www.polytechnique.edu/), and my ongoing master's studies in the [MVA](https://www.master-mva.com/) program, I enjoy addressing challenging problems in the field of AI. Beyond academics, you can find me pursuing my hobbies, which include calisthenics, playing the guitar, cycling, meditation, and reading.

News
======
## *15 October 2023*: New Paper Available on arXiv
You can access the full paper on arXiv by clicking [here](https://arxiv.org/abs/2310.09804).

I'm excited to share that our latest research paper, titled "Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates" is now available on arXiv. This work was conducted during my internship at [KAUST](https://kaust.edu.sa) in the group of [Peter Richtarik](https://richtarik.org).

**Paper Overview**
In our recent paper, we focus on Byzantine-robust learning with compression, a crucial element for distributed optimization in collaborative learning scenarios. We introduce Byz-DASHA-PAGE, a novel Byzantine-robust method with compression, which offers superior convergence rates, smaller neighborhood sizes in heterogeneous cases, and increased tolerance to Byzantine workers compared to the state-of-the-art Byz-VR-MARINA. Additionally, we present Byz-EF21, the first Byzantine-robust method with communication compression and error feedback.