---
title: "Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates"
collection: publications
permalink: /publication/LLMs_Math
excerpt: 'We study the ability of large language models to learn specific mathematical rules.'
date: 2024-10-15
venue: 'Neurips Math-AI 2024'
paperurl: 'https://arxiv.org/pdf/2310.09804.pdf'
---
In this paper, we study the ability of large language models to learn specific
mathematical rules such as distributivity or simplifying equations. We present
an empirical analysis of their ability to generalize these rules, as well as to reuse
them in the context of word problems. For this purpose, we provide a rigorous
methodology to build synthetic data incorporating such rules, and perform fine-
tuning of large language models on such data. Our experiments show that our
model can learn and generalize these rules to some extent, as well as suitably reuse
them in the context of word problems.

[Download paper here](https://arxiv.org/pdf/2410.16973.pdf)